### ELU

- 레이어

  ELU(Exponential Linear Unit)는 LeakyReLU와 비슷하게 음수 입력에 대해 기울기를 남기면서도, 출력이 0에 수렴하도록 합니다. 이로 인해 ReLU보다 더 부드러운 경사면을 형성하여 안정적인 학습이 가능합니다.

- 파라미터

  - `alpha` : 음수 입력에 대한 스케일링 값으로, 0 이상의 실수 값을 지정합니다.

- 팁
  1. **부드러운 비선형성 제공** : ELU는 ReLU보다 더 부드러운 비선형성을 제공하여 더 안정적인 학습을 돕습니다.
  2. **기울기 소실 방지** : 음수 영역에서 기울기를 남겨놓아 죽은 뉴런 현상이 발생할 가능성이 낮습니다.
  3. **적절한 alpha 설정** : 일반적으로 `alpha`는 1로 설정되지만, 문제에 따라 미세 조정할 수 있습니다.
