### GELU

- 레이어

  Gaussian Error Linear Unit(GELU)는 활성화 함수로, 입력 값을 Gaussian 분포에 따라 스무스하게 변환하여 학습을 안정화합니다. ReLU와 달리 입력 값의 변화에 따라 비선형적 반응을 제공합니다.

- 파라미터

  - N/A

- 팁
  1. **부드러운 비선형성 제공** : Gaussian 분포에 따른 비선형성을 제공해, 자연스러운 활성화 변화를 제공합니다.
  2. **Transformer 모델에서 사용** : 최신 NLP 모델에서 활성화 함수로 많이 사용되며, 특히 Transformer 모델에서 좋은 성능을 보입니다.
  3. **정확한 학습 지원** : ReLU보다 부드러운 비선형성을 제공해 학습의 안정성을 높일 수 있습니다.
